# Sparse Attention Kernel

Triton Attention kernel that takes advantage of block-sparse attention masks.

### Problem
FlashAttention has limited support for block-sparse attention masks - it only has special cases hardcoded for causal and sliding window attention.
When you pass a tensor as the attention mask, FlashAttention computes attention over every block, whether it's fully masked, fully unmasked, or partially masked, leading to wasted computation.

### Solution
By working out which blocks will be totally masked out in advance, we can skip over these in the attention kernel. We can also skip applying the mask to fully unmasked blocks.

Using a Triton kernel to parse the mask is super quick. I'm ignoring the mask materialisation costs for now - an improvement would be to define the masks algebraically and pass that logic into the kernel somehow...

FlexAttention accounts for sparsity when you algebraically define the mask, but I wanted to write a kernel that took a tensor mask (as flashattention does) and dynamically masks based on the entries of the tensor.


### How does it work?

For a mask, for each block of queries, and each kv block, calculate if the corresponding mask is either:
- Fully unmasked
- Partially masked
- Fully masked (skip)

For each query block, we need to calculate:
- how many unmasked kv blocks there are, and their indices
- how many partially masked kv blocks there are, and their indices

Then, inside the attention kernel, we do two passes for each query block:
1. Fully unmasked kv blocks, we don't even need to apply the mask here (gets ~20% speed-up)
2. Partially masked kv blocks

The logic is very similar to how you partition the blocks for a causal mask / sliding mask, but with an extra load before we load each kv block in order to load the index of that block (a bit like paged attention)

This approach is mostly targetted at very block-y masks, like document masking, sliding/causal masks, prefixlm etc.
It will also happily handle compositions of these, as there is no assumption made on the mask.


### Performance

Benchmarked on A100

B, H, S, D = 8, 32, 4096, 128

Masks generated by tiling SxS grid with blocks of size 32x32, each masked out with a probability $p$. Additionally, there's a flat 10% chance for a block to be internally randomly masked (to simulate mask boundaries not perfectly aligning with the grid)

I'm assuming that the mask materialisation costs get amortized over the layers, so not including that in the timing.
It's roughly 0.5ms at this shape, ~10% of the time taken to do an unmasked forwards pass.

Could be interesting to compare this to FlexAttention's materialisation cost + try an algebraic approach.
e.g. when it's causal/sliding, you can calculate the mask within the kernel - wonder how to express this for other families of masks.


![Performance Results](/assets/perf.png)

Still some work to get to FlexAttention

But significant improvement on FlashAttention for sparse patterns!



### What could come next

1. Algebraically defined masks inlined to the kernel to avoid materialisation + memory costs.
2. Changing blocksizes / offsets within the kernel to maximise the number of totally masked-out blocks.
3. Backwards pass + decode kernels


